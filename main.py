import re
import asyncio
import logging
import json
from urllib.parse import urlparse

import aiohttp
from bs4 import BeautifulSoup
from jinja2 import Template

from astrbot.api.event import filter, AstrMessageEvent
from astrbot.api.star import Context, Star, register
from astrbot.api.provider import ProviderRequest
from astrbot.api import logger, AstrBotConfig

@register("astrbot_plugin_link_context_reader", "YourName", "æ™ºèƒ½é“¾æ¥å†…å®¹è¯»å–ä¸LLMä¸Šä¸‹æ–‡å¢å¼ºæ’ä»¶", "1.0.0", "https://github.com/YourName/astrbot_plugin_link_context_reader")
class LinkContextReader(Star):
    def __init__(self, context: Context, config: AstrBotConfig):
        super().__init__(context)
        self.config = config
        
        # ç¼–è¯‘ URL åŒ¹é…æ­£åˆ™
        self.url_pattern = re.compile(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:[/?]\S*)?')
        
        # éŸ³ä¹å¹³å°åŸŸåç‰¹å¾
        self.music_domains = ['music.163.com', 'y.qq.com', 'kugou.com', 'kuwo.cn']
        # ç¤¾äº¤å¹³å°åŸŸåç‰¹å¾
        self.social_domains = ['zhihu.com', 'weibo.com', 'weibo.cn', 'xiaohongshu.com', 'lofter.com']

    @filter.command("link_reader_status")
    async def link_reader_status(self, event: AstrMessageEvent):
        """æŸ¥çœ‹å½“å‰é“¾æ¥è§£ææœåŠ¡çš„çŠ¶æ€"""
        status = "å¼€å¯" if self.config.get("enable_auto_parse", True) else "å…³é—­"
        blacklist = self.config.get("blacklisted_domains", [])
        
        msg = (
            f"ğŸ”— é“¾æ¥è§£ææœåŠ¡çŠ¶æ€: {status}\n"
            f"ğŸŒ å½“å‰é»‘åå•åŸŸåæ•°: {len(blacklist)}\n"
            f"ğŸ“ å†…å®¹æˆªæ–­é•¿åº¦: {self.config.get('max_content_length', 1500)}\n"
            f"â±ï¸ è¯·æ±‚è¶…æ—¶æ—¶é—´: {self.config.get('request_timeout', 10)}ç§’"
        )
        yield event.plain_result(msg)

    @filter.command("toggle_link_reader")
    async def toggle_link_reader(self, event: AstrMessageEvent):
        """å¼€å¯æˆ–å…³é—­é“¾æ¥è‡ªåŠ¨è§£æåŠŸèƒ½"""
        current_status = self.config.get("enable_auto_parse", True)
        new_status = not current_status
        self.config["enable_auto_parse"] = new_status
        self.config.save_config() # ä¿å­˜é…ç½®
        
        status_str = "å·²å¼€å¯" if new_status else "å·²å…³é—­"
        yield event.plain_result(f"ğŸ”— é“¾æ¥è‡ªåŠ¨è§£æåŠŸèƒ½{status_str}ã€‚")

    @filter.on_llm_request()
    async def on_llm_request(self, event: AstrMessageEvent, req: ProviderRequest):
        """
        æ‹¦æˆª LLM è¯·æ±‚ï¼Œæ£€æµ‹ URL å¹¶æ³¨å…¥å†…å®¹
        """
        # 1. æ£€æŸ¥å¼€å…³
        if not self.config.get("enable_auto_parse", True):
            return

        # 2. æå– URL
        # æ³¨æ„ï¼šè¿™é‡Œä¼˜å…ˆæ£€æŸ¥ event.message_strï¼Œå› ä¸ºå®ƒåŒ…å«åŸå§‹ç”¨æˆ·æ¶ˆæ¯
        text = event.message_str or ""
        urls = self.url_pattern.findall(text)
        
        if not urls:
            return

        # åªå¤„ç†ç¬¬ä¸€ä¸ª URLï¼Œé¿å…è¿‡å¤šè¯·æ±‚
        target_url = urls[0]
        
        # 3. æ£€æŸ¥é»‘åå•
        domain = urlparse(target_url).netloc
        blacklist = self.config.get("blacklisted_domains", [])
        if any(d in domain for d in blacklist):
            logger.info(f"[LinkReader] Domain {domain} is blacklisted, skipping.")
            return

        # 4. è·¯ç”±å¤„ç†ä¸å†…å®¹è·å–
        logger.info(f"[LinkReader] Detected URL: {target_url}, start fetching...")
        try:
            parse_result = await self._fetch_and_parse(target_url)
            
            if not parse_result:
                return

            # 5. æ¸²æŸ“æ³¨å…¥æ¨¡æ¿
            template_str = self.config.get("injection_template", "")
            if not template_str:
                # é»˜è®¤æ¨¡æ¿
                template_str = "ã€ç³»ç»Ÿæ£€æµ‹åˆ°æ¶ˆæ¯ä¸­åŒ…å«é“¾æ¥ï¼Œå·²è‡ªåŠ¨è¯»å–å†…å®¹ã€‘\né“¾æ¥æ ‡é¢˜ï¼š{{title}}\né“¾æ¥å†…å®¹æ‘˜è¦ï¼š\n{{content}}\n\nè¯·åŸºäºä»¥ä¸Šé“¾æ¥å†…å®¹ï¼Œå›å¤ç”¨æˆ·çš„æ¶ˆæ¯ï¼š\n"
            
            tmpl = Template(template_str)
            injection_text = tmpl.render(
                title=parse_result.get("title", "æ— æ ‡é¢˜"),
                url=target_url,
                content=parse_result.get("content", "")
            )

            # 6. æ³¨å…¥åˆ° System Prompt
            # ä¹Ÿå¯ä»¥é€‰æ‹©è¿½åŠ åˆ° req.text æˆ– context ä¸­ï¼Œè¿™é‡Œé€‰æ‹©è¿½åŠ åˆ° system_prompt ä»¥ä½œä¸ºèƒŒæ™¯çŸ¥è¯†
            original_sys_prompt = req.system_prompt or ""
            req.system_prompt = f"{original_sys_prompt}\n\n{injection_text}"
            
            logger.info(f"[LinkReader] Successfully injected content from {target_url}")

        except Exception as e:
            logger.error(f"[LinkReader] Error processing URL {target_url}: {e}")
            # å‡ºé”™ä¸ä¸­æ–­æµç¨‹ï¼Œè®© LLM ç»§ç»­å¤„ç†åŸå§‹æ¶ˆæ¯

    async def _fetch_and_parse(self, url: str) -> dict:
        """æ ¸å¿ƒè·å–ä¸è§£æé€»è¾‘"""
        timeout = self.config.get("request_timeout", 10)
        ua = self.config.get("user_agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        domain = urlparse(url).netloc
        cookies = {}
        
        # å¹³å°ç‰¹å®š Cookie å¤„ç†
        platform_cookies = self.config.get("platform_cookies", {})
        if "zhihu" in domain and platform_cookies.get("zhihu_cookie"):
            cookies["z_c0"] = platform_cookies["zhihu_cookie"]
        elif "weibo" in domain and platform_cookies.get("weibo_cookie"):
            cookies["SUB"] = platform_cookies["weibo_cookie"]
        elif "xiaohongshu" in domain and platform_cookies.get("xiaohongshu_cookie"):
            cookies["web_session"] = platform_cookies["xiaohongshu_cookie"]

        features = self.config.get("features_switch", {})
        
        async with aiohttp.ClientSession(cookies=cookies) as session:
            try:
                async with session.get(url, headers={"User-Agent": ua}, timeout=timeout) as resp:
                    if resp.status != 200:
                        logger.warning(f"[LinkReader] Fetch failed: {resp.status}")
                        return None
                    
                    # é’ˆå¯¹éƒ¨åˆ†ç¼–ç é—®é¢˜ï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹ï¼Œé»˜è®¤ä¸º utf-8
                    html = await resp.text(errors='ignore')
                    
                    # è·¯ç”±åˆ†å‘
                    if any(d in domain for d in self.music_domains):
                        if not features.get("search_lyrics", True): return None
                        return await self._parse_music(html, url)
                    
                    elif any(d in domain for d in self.social_domains):
                        if not features.get("parse_social_media", True): return None
                        return await self._parse_social(html, url)
                    
                    else:
                        if not features.get("parse_generic_web", True): return None
                        return await self._parse_generic(html, url)
                        
            except asyncio.TimeoutError:
                logger.warning(f"[LinkReader] Fetch timeout for {url}")
                return None
            except Exception as e:
                logger.error(f"[LinkReader] Request error: {e}")
                return None

    async def _parse_generic(self, html: str, url: str) -> dict:
        """é€šç”¨ç½‘é¡µè§£æ"""
        soup = BeautifulSoup(html, 'lxml')
        
        # ç§»é™¤å¹²æ‰°å…ƒç´ 
        for tag in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript', 'svg']):
            tag.decompose()
            
        title = soup.title.string.strip() if soup.title else "æ— æ ‡é¢˜"
        
        # æå–æ­£æ–‡ï¼šä¼˜å…ˆæå– article æ ‡ç­¾ï¼Œå¦åˆ™æå–æ‰€æœ‰ p æ ‡ç­¾
        content = ""
        article = soup.find('article')
        if article:
            content = article.get_text(separator='\n', strip=True)
        else:
            # ç®€å•çš„æ–‡æœ¬å¯†åº¦æå–ç­–ç•¥
            paragraphs = [p.get_text(strip=True) for p in soup.find_all('p') if len(p.get_text(strip=True)) > 10]
            content = "\n".join(paragraphs)
            
        return self._format_result(title, content)

    async def _parse_social(self, html: str, url: str) -> dict:
        """ç¤¾äº¤åª’ä½“è§£æ (åŸºäº OpenGraph åè®®ä¼˜å…ˆ)"""
        soup = BeautifulSoup(html, 'lxml')
        
        title = "ç¤¾äº¤åª’ä½“åˆ†äº«"
        content = ""
        
        # å°è¯• OpenGraph åè®®æå– (é€šç”¨æ€§å¼ºï¼Œé€‚ç”¨äºçŸ¥ä¹ã€å¾®åšç­‰æ¸²æŸ“å‰çš„é¡µé¢)
        og_title = soup.find("meta", property="og:title")
        if og_title:
            title = og_title.get("content", "").strip()
            
        og_desc = soup.find("meta", property="og:description")
        if og_desc:
            content = og_desc.get("content", "").strip()
            
        # å¦‚æœ OpenGraph æ²¡æå–åˆ°å†…å®¹ï¼Œå°è¯• fallback åˆ° body text
        if not content:
            # é’ˆå¯¹çŸ¥ä¹çš„ç‰¹å®šå¤„ç† (çŸ¥ä¹æœ‰æ—¶å°†å†…å®¹æ”¾åœ¨ script id="js-initialData" ä¸­ï¼Œè¿™é‡Œä»…åšç®€å•æ–‡æœ¬æå–)
            # å®é™…ç”Ÿäº§ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æé€»è¾‘
            content = soup.get_text(separator='\n', strip=True)[:500] + "..."
            
        return self._format_result(title, content)

    async def _parse_music(self, html: str, url: str) -> dict:
        """éŸ³ä¹é“¾æ¥è§£æ"""
        soup = BeautifulSoup(html, 'lxml')

        # 1. æå–åŸå§‹æ ‡é¢˜å¹¶æ¸…æ´—
        raw_title = soup.title.string.strip() if soup.title else "æœªçŸ¥éŸ³ä¹"
        # ç§»é™¤å¹³å°åç¼€ï¼Œä»…ä¿ç•™ æ­Œæ‰‹ - æ­Œæ›²å éƒ¨åˆ†
        clean_title = raw_title.split('(è±†ç“£)')[0].split('- ç½‘æ˜“äº‘')[0].split('- QQéŸ³ä¹')[0].strip()
        
        # 2. å°è¯•ä» meta æ ‡ç­¾è·å–æ›´ç²¾å‡†çš„å…³é”®è¯ (og:title é€šå¸¸åŒ…å«æ›´çº¯å‡€çš„ æ­Œæ›²-æ­Œæ‰‹ ä¿¡æ¯)
        og_title = soup.find("meta", property="og:title")
        search_keyword = og_title.get("content", "") if og_title else clean_title
        
        # 3. æ„é€ åŠŸèƒ½æ€§å†…å®¹
        content = f"ğŸµ è¯†åˆ«åˆ°éŸ³ä¹ï¼š{search_keyword}\n"
        content += "---"
    
       # æ„é€ ç²¾å‡†æœç´¢é“¾æ¥ (ä»¥ Google/ç™¾åº¦ æˆ– å‚ç›´ç¤¾åŒºä¸ºä¾‹)
        # ä½¿ç”¨ quote ç¡®ä¿ URL ç¼–ç å®‰å…¨
        from urllib.parse import quote
        encoded_query = quote(search_keyword)
    
        content += f"\nğŸ” [æœç´¢æ­Œè¯]ï¼šhttps://www.google.com/search?q={encoded_query}+æ­Œè¯"
        content += f"\nğŸ’¬ [æŸ¥çœ‹è¯„ä»·]ï¼šhttps://search.douban.com/music/subject_search?search_text={encoded_query}"
        content += f"\nğŸ§ [å¹³å°æ£€ç´¢]ï¼šhttps://music.163.com/#/search/m/?s={encoded_query}"
    
        content += "\n\n(æç¤ºï¼šç”±äºç‰ˆæƒä¿æŠ¤ï¼Œè¯¦ç»†æ­Œè¯ä¸æ·±åº¦ä¹è¯„è¯·ç‚¹å‡»ä¸Šæ–¹é“¾æ¥è·³è½¬æŸ¥çœ‹)"

        return self._format_result(search_keyword, content)
        

    def _format_result(self, title: str, content: str) -> dict:
        """æ ¼å¼åŒ–å¹¶æˆªæ–­ç»“æœ"""
        max_len = self.config.get("max_content_length", 1500)
        
        if len(content) > max_len:
            content = content[:max_len] + f"\n...(å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³{max_len}å­—)"
            
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        content = re.sub(r'\n\s*\n', '\n', content)
        
        return {
            "title": title,
            "content": content
        }